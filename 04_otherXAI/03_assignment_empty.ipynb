{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a97cd228",
   "metadata": {},
   "source": [
    "# Assignment 03\n",
    "\n",
    "### Deadline: March 6th 2025, 10:00 AM\n",
    "\n",
    "### Deliverables:\n",
    "Please submit, via Canvas, a **zip file** including the following:\n",
    "- a .ipynb file (you can use this one) with all of your code included -- `03_assignment_surname1_surname2.ipynb`\n",
    "- a compiled .html file of your .ipynb which includes all of the output -- `03_assignment_surname1_surname2.html`\n",
    "- a pdf file with your written answers to the questions -- `03_assignment_surname1_surname2.pdf`\n",
    "\n",
    "Make sure to follow the naming convention indicated above. The zip name can be named `03_assignment_surname1_surname2`.\n",
    "\n",
    "Make sure to annotate your code. We may substract points if code is not annotated and unclear.\n",
    "\n",
    "To be able to run this notebook, run the following commands in your virtual environment: \n",
    "\n",
    "```\n",
    "# navigate to your folder with the virtual environment OR create a new one\n",
    "cd <.../.../.../04_otherXAI>\n",
    "\n",
    "# activate the virtual environment \n",
    "source <venv_name>/bin/activate\n",
    "\n",
    "# or on Windows: \n",
    "<venv_nane>/Scripts/activate\n",
    "\n",
    "# install a package to for visualisation. My version of mlxtend is 0.23.4, of gplearn it's 0.4.2, and of pysr it's 0.19.4\n",
    "pip install mlxtend gplearn pysr\n",
    "\n",
    "# install the package for robust counterfactuals\n",
    "git clone https://github.com/donato-maragno/robust-CE.git\n",
    "\n",
    "cd robust-CE\n",
    "\n",
    "pip install .\n",
    "\n",
    "# close the virtual environment\n",
    "deactivate\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c500784",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We are using the Statlog (German Credit Data) dataset. The German Credit dataset classifies people described by a set of 20 features as good or bad credit risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd6395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3520e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data \n",
    "X_train = pd.read_csv('../datasets/credit/encoded_credit_X_train.csv')\n",
    "y_train = pd.read_csv('../datasets/credit/credit_y_train.csv')\n",
    "\n",
    "# test data\n",
    "X_test = pd.read_csv('../datasets/credit/encoded_credit_X_test.csv')\n",
    "y_test = pd.read_csv('../datasets/credit/credit_y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ff4ec",
   "metadata": {},
   "source": [
    "# Robust Counterfactuals\n",
    "\n",
    "## 1. Preparation\n",
    "\n",
    "(5 points)\n",
    "\n",
    "For demonstration purposes we will train a model to predict good/bad credit risk using only `duration` and `credit_amount`, for which we will then create robust counterfactual explanations.\n",
    "\n",
    "**1.1** Select only the features `duration` and `credit_amount` from the dataset. Then, apply `MinMaxScaler` from `sklearn` to scale the data to be between 0 and 1.\n",
    "\n",
    "**1.2** Train a decision tree using `DecisionTreeClassifier` from `scikit-learn`. Use `max_depth = 10` and `random_state = 1`.\n",
    "\n",
    "**1.3** Inspect the model and evaluate performance and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86235e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ****************************************************************************\n",
    "##                           adjust code here for Q 1.1, Q 1.2 and Q 1.3\n",
    "## ****************************************************************************\n",
    "\n",
    "# -------------\n",
    "# select only duration and credit_amount\n",
    "# -------------\n",
    "...\n",
    "\n",
    "# -------------\n",
    "# scale the data\n",
    "# -------------\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "## ****************************************************************************\n",
    "##                           adjust code here for Q 1.1 and Q 1.2\n",
    "## ****************************************************************************\n",
    "\n",
    "# -------------\n",
    "# train model\n",
    "# -------------\n",
    "\n",
    "# import decision tree class from sklearn\n",
    "...\n",
    "\n",
    "# initialize and fit classifier\n",
    "...\n",
    "\n",
    "# predict on test set \n",
    "...\n",
    "\n",
    "\n",
    "# -------------\n",
    "# inspect performance\n",
    "# -------------\n",
    "\n",
    "# import and print metrics\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae641d",
   "metadata": {},
   "source": [
    "## 2. Optimization\n",
    "\n",
    "(10 points)\n",
    "\n",
    "We are using sample 54 as example datapoint to explain. The prediction for this sample is 0, so we want to know how to change the features such that the prediction would be 1.\n",
    "\n",
    "**2.1** Run `rce.generate()` to generate a region of counterfactual explanations, i.e. robust counterfactuals for the sample at index 54. Follow the readme file on the repo: https://github.com/donato-maragno/robust-CE/. Use the iterative approach. As uncertainty set, we are using the L-infinity norm. Choose the radius of the set to be 0.05. \n",
    "\n",
    "**2.2** The solution returns the center of the set. Given this and the radius of the uncertainty set, calculate the lower and upper bound of the box. We want to save those in a dataframe, balled `bounds`, where the columns are `duration` and `credit_amount`. The first row should show the lower bound of each feature respectively, and the second row should show the upper bound, like in this example:\n",
    "\n",
    "<div>\n",
    "<img src=\"../img/bounds example.png\" width=\"250\" align='left'/>\n",
    "</div>\n",
    "\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "Once you have done that you can apply reverse scaling to this dataframe to inspect the feature ranges in the original feature space.\n",
    "\n",
    "**2.3** Plot the decision areas of the decision tree, the factual instance and the box of the counterfactuals. You can use the code provided below. (Note: use the scaled data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab46376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------\n",
    "# choose factual instance\n",
    "# -------------\n",
    "idx = 54\n",
    "print('The actual label of sample {0} is {1}.'.format(idx, y_test.iloc[idx,-1]))\n",
    "print('The predicted label of sample {0} is {1}.'.format(idx, y_pred.loc[idx]))\n",
    "# define the factual instance\n",
    "u = pd.DataFrame(np.array(X_test.loc[idx,:]).reshape(1,-1), columns=X_test.columns)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db67c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ****************************************************************************\n",
    "##                           adjust code here for Q 2.1\n",
    "## ****************************************************************************\n",
    "\n",
    "# import robust CE package\n",
    "import rce\n",
    "\n",
    "# assign all parameters\n",
    "...\n",
    "\n",
    "# run model\n",
    "...\n",
    "\n",
    "\n",
    "## ****************************************************************************\n",
    "##                           adjust code here for Q 2.2\n",
    "## ****************************************************************************\n",
    "\n",
    "# create dataframe with lower and upper bounds of the box\n",
    "\n",
    "...\n",
    "\n",
    "bounds = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec33f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ****************************************************************************\n",
    "##                           code for Q 2.1 (ideally no need to adjust)\n",
    "## ****************************************************************************\n",
    "\n",
    "# -------------\n",
    "# visualise results\n",
    "# -------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# features to plot\n",
    "features_to_plot = {'x':'duration','y':'credit_amount'}\n",
    "\n",
    "# details for plotting datapoints\n",
    "scatter_kwargs = {'s': 80, 'edgecolor': None, 'alpha': 0.6}\n",
    "contourf_kwargs = {'alpha': 0.2}\n",
    "scatter_highlight_kwargs = {'s': 120, 'label': 'factual instance', 'alpha': 1.0, 'c':'yellow', 'edgecolor':'darkgrey'}\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(1,figsize=(10, 10))\n",
    "\n",
    "# plot decision areas\n",
    "plot_decision_regions(np.array(X_test), np.array(y_test).squeeze(), \n",
    "                      clf=clf, \n",
    "                      legend=2, # top left \n",
    "                      X_highlight=np.array(u), # factual instance\n",
    "                      zoom_factor=1, # level of detail\n",
    "                      scatter_kwargs=scatter_kwargs, # size, color, etc. of datapoints\n",
    "                      contourf_kwargs=contourf_kwargs,\n",
    "                      scatter_highlight_kwargs=scatter_highlight_kwargs) # size, color, etc. of factual instance\n",
    "\n",
    "# adding robust CE box\n",
    "ax.add_patch(Rectangle((bounds.loc['lower_bound'][features_to_plot['x']], \n",
    "                        bounds.loc['lower_bound'][features_to_plot['y']]), \n",
    "                       rho*2, rho*2,\n",
    "                       color='green',\n",
    "                       edgecolor = 'black',\n",
    "                       fill=True,\n",
    "                       lw=0.5,\n",
    "                       alpha=0.6))\n",
    "\n",
    "# Adding axes annotations\n",
    "plt.xlabel(f'scaled {features_to_plot[\"x\"]}')\n",
    "plt.ylabel(f'scaled {features_to_plot[\"y\"]}')\n",
    "plt.title('Decision areas of the decision tree')\n",
    "plt.xlim(-0.05, 0.8)\n",
    "plt.ylim(-0.05, 0.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a35e62",
   "metadata": {},
   "source": [
    "## 3. Discussion \n",
    "\n",
    "(10 points)\n",
    "\n",
    "**3.1** Describe the results â€“ what does the function return, what does the resulting box tell us, how can we formulate an explanation from it? Use the results for sample at index 54 to support your answer.\n",
    "\n",
    "**3.2** In some situations the iterative approach may take quite long. In our paper we describe a heuristic variant that solves the problem much quicker. Describe this heuristic and explain why it is quicker, and what trade-off we might run into. You can find the paper here: https://pubsonline.informs.org/doi/abs/10.1287/ijoc.2023.0153?journalCode=ijoc. It is also uploaded to the Github repo.\n",
    "\n",
    "**3.3** Run the code again, now using the heuristic. Explain the result and compare the result with the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf88a0",
   "metadata": {},
   "source": [
    "---\n",
    "# Symbolic regression\n",
    "\n",
    "In this assignment you will again compare GPLearn with PySR by choosing an equation from the AI Feynman Physics-Inspired Database of Equations (Udrescu & Tegmark, 2019). You will download the dataset and load it into this assignment notebook. \n",
    "\n",
    "Open the AI Feynman database of equations. You can find it here: https://space.mit.edu/home/tegmark/aifeynman.html. In Table IV of the provided paper, or in folder `'Feynman_with_units.tar.gz'`, look for a monomial with at least four features. You can download the dataset that belongs to the chosen equation from `'Feynman_with_units.tar.gz'`. \n",
    "\n",
    "**IMPORTANT:** The dataset is quite big and you may have problems downloading it. Hence, we have uploaded a subset of the equations to Canvas (under week 4). You can select an **equation with 4 features** from this subset. Make sure to download the folder and place it in the datasets folder of the repo :) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65270773",
   "metadata": {},
   "source": [
    "## 4. Feynman Symbolic Regression Database\n",
    "\n",
    "(15 points)\n",
    "\n",
    "**4.1** Create a dataset of your chosen equation as a DataFrame. Take only the first 30 samples of the dataset of your chosen equation. This requires you to download the dataset from the AI Feynman webpage first. Remember to choose an equation with at least four features.\n",
    "\n",
    "**4.2** Use GPLearn to find the equation you chose. Motivate the chosen values for `generations` and `population_size`.\n",
    "\n",
    "**4.3** Another package that fits a symbolic regression model is `PySR`. Install it using `pip install pysr` if you haven't done that yet. Fit their `PySRRegressor` to the dataset. Follow the quick start instructions on their Github repo: https://github.com/MilesCranmer/PySR. The complete documentation can be found here: https://astroautomata.com/PySR/\n",
    "\n",
    "- for the arguments `binary_operators` and `unary_operators` you can use the default values\n",
    "- make sure that your model is reproducible. Read the documentation on how to do that for this model. \n",
    "\n",
    "**4.4** Create a synthetic test dataset based on the true equation consisting of 10 samples and use both found equations to predict the corresponding target value for each sample. Provide the total L1-loss for both GPLearn PySR. Recall that you can calculate the total L1-loss by simply summing up the differences between the actual output and predicted output for each sample. What are your conclusions about both methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8bcc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ****************************************************************************\n",
    "##                        adjust/add code here for Q 4.1\n",
    "## ****************************************************************************\n",
    "\n",
    "# load the dataset for the equation that you chose\n",
    "# you can use the code below\n",
    "\n",
    "# choose equation\n",
    "equation_label = '...'\n",
    "\n",
    "file_path = f'../datasets/feynman_equations/{equation_label}'\n",
    "data = pd.read_csv(file_path, header=None, sep = ' ').iloc[:,:-1] # remove last column because feynman datasets include empty space in the last column\n",
    "data.columns = ['x1','x2','x3','x4','output'] # add column names\n",
    "\n",
    "# select only first 30 rows\n",
    "...\n",
    "\n",
    "# separate input features and output\n",
    "X = ...\n",
    "y = ...\n",
    "\n",
    "\n",
    "## ****************************************************************************\n",
    "##                        adjust/add code here for Q 4.3\n",
    "## ****************************************************************************\n",
    "\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "# fit SymbolicRegressor\n",
    "...\n",
    "\n",
    "# get equation\n",
    "...\n",
    "\n",
    "\n",
    "## ****************************************************************************\n",
    "##                        adjust/add code here for Q 4.4\n",
    "## ****************************************************************************\n",
    "\n",
    "from pysr import PySRRegressor\n",
    "\n",
    "# fit PySRRegressor\n",
    "...\n",
    "\n",
    "# get best equation\n",
    "...\n",
    "\n",
    "\n",
    "## ****************************************************************************\n",
    "##                        adjust/add code here for Q 4.5\n",
    "## ****************************************************************************\n",
    "\n",
    "# Creating dataset according to the equation\n",
    "\n",
    "random_state = 42\n",
    "nof_samples = 10\n",
    "dim = 4\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# generate data\n",
    "...\n",
    "\n",
    "# calculate y_actual using the true equation & predict y using both equations returned by the models\n",
    "...\n",
    "\n",
    "# calculate loss for both models\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week4_test",
   "language": "python",
   "name": "week4_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
